{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DROID-SLAM Processing for Stack Sessions\n",
    "\n",
    "Process raw capture sessions (ultrawide camera, no ARKit) through DROID-SLAM to get 6DoF poses.\n",
    "\n",
    "**Requirements:** Google Colab Pro with GPU runtime (A100/V100, 11GB+ VRAM)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Upload session from Google Drive\n",
    "2. Load RGB frames + camera intrinsics (calib.txt)\n",
    "3. Run DROID-SLAM inference → SE3 poses\n",
    "4. Apply metric scale correction\n",
    "5. Write poses.json back to session\n",
    "6. Download or sync back to Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DROID-SLAM and dependencies\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 -q\n",
    "!pip install lietorch pytorch_scatter -q\n",
    "!pip install opencv-python-headless scipy numpy pillow -q\n",
    "\n",
    "# Clone and install DROID-SLAM\n",
    "!git clone https://github.com/princeton-vl/DROID-SLAM.git /content/DROID-SLAM 2>/dev/null || true\n",
    "!cd /content/DROID-SLAM && pip install -e . -q\n",
    "\n",
    "# Download pretrained weights\n",
    "!mkdir -p /content/DROID-SLAM/checkpoints\n",
    "!gdown --id 1PpqVt1H4maBa_GbPJp4NwxRsd9jk-elh -O /content/DROID-SLAM/checkpoints/droid.pth 2>/dev/null || echo 'Download weights manually if gdown fails'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Select Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# Set the path to your session directory on Google Drive\nDRIVE_SESSIONS_DIR = Path('/content/drive/MyDrive/stack_sessions')"
  },
  {
   "cell_type": "code",
   "source": "# Unzip uploaded session archives (skip already-extracted ones)\nimport subprocess\nif DRIVE_SESSIONS_DIR.exists():\n    zips = sorted(DRIVE_SESSIONS_DIR.glob('*.zip'))\n    print(f\"Found {len(zips)} zip files\")\n    for z in zips:\n        session_dir = DRIVE_SESSIONS_DIR / z.stem\n        if session_dir.exists():\n            print(f\"  {z.name}: already extracted, skipping\")\n        else:\n            print(f\"  {z.name}: extracting...\")\n            subprocess.run(['unzip', '-q', '-o', str(z), '-d', str(DRIVE_SESSIONS_DIR)], check=True)\n    print(\"Done!\")\nelse:\n    print(f\"Upload zipped sessions to Google Drive: My Drive/stack_sessions/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# List available sessions\nif DRIVE_SESSIONS_DIR.exists():\n    sessions = sorted([d for d in DRIVE_SESSIONS_DIR.iterdir() if d.is_dir() and d.name.startswith('session_')])\n    print(f\"Found {len(sessions)} sessions:\")\n    for s in sessions:\n        meta_file = s / 'metadata.json'\n        if meta_file.exists():\n            with open(meta_file) as f:\n                meta = json.load(f)\n            source = meta.get('captureSource', 'iphone_arkit')\n            processed = meta.get('slamProcessed', False)\n            n_frames = meta.get('rgbFrameCount', '?')\n            status = 'done' if processed else ('arkit' if source == 'iphone_arkit' else 'needs SLAM')\n            print(f\"  {s.name}: {n_frames} frames, source={source}, status={status}\")\n        else:\n            print(f\"  {s.name}: no metadata\")\nelse:\n    print(f\"No sessions found at {DRIVE_SESSIONS_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select session to process\n",
    "SESSION_NAME = 'session_2026-02-20_143000'  # <-- Change this\n",
    "SESSION_DIR = DRIVE_SESSIONS_DIR / SESSION_NAME\n",
    "\n",
    "assert SESSION_DIR.exists(), f\"Session not found: {SESSION_DIR}\"\n",
    "\n",
    "# Load metadata\n",
    "with open(SESSION_DIR / 'metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Frames & Intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RGB frames\n",
    "rgb_dir = SESSION_DIR / 'rgb'\n",
    "frame_paths = sorted(rgb_dir.glob('*.jpg'))\n",
    "print(f\"Found {len(frame_paths)} RGB frames\")\n",
    "\n",
    "# Preview first frame\n",
    "import matplotlib.pyplot as plt\n",
    "first_frame = cv2.imread(str(frame_paths[0]))\n",
    "first_frame = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(first_frame)\n",
    "plt.title(f\"Frame 0: {first_frame.shape[1]}x{first_frame.shape[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Load intrinsics\n",
    "calib_file = SESSION_DIR / 'calib.txt'\n",
    "if calib_file.exists():\n",
    "    parts = calib_file.read_text().strip().split()\n",
    "    intrinsics = np.array([float(x) for x in parts[:4]])\n",
    "    print(f\"Intrinsics from calib.txt: fx={intrinsics[0]:.1f} fy={intrinsics[1]:.1f} cx={intrinsics[2]:.1f} cy={intrinsics[3]:.1f}\")\n",
    "else:\n",
    "    # Default estimate for ultrawide at 480x360\n",
    "    intrinsics = np.array([300.0, 300.0, 240.0, 180.0])\n",
    "    print(f\"No calib.txt — using default estimate: {intrinsics}\")\n",
    "    print(\"DROID-SLAM will auto-calibrate (opt_intr=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run DROID-SLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/content/DROID-SLAM')\n",
    "from droid_slam import Droid\n",
    "\n",
    "# Check if we need intrinsic optimization\n",
    "opt_intr = not calib_file.exists()\n",
    "\n",
    "# Initialize DROID-SLAM\n",
    "droid = Droid(\n",
    "    image_size=[360, 480],  # H, W\n",
    "    intrinsics=intrinsics,\n",
    "    weights='/content/DROID-SLAM/checkpoints/droid.pth',\n",
    "    opt_intr=opt_intr,\n",
    "    buffer=512,\n",
    "    beta=0.3,\n",
    ")\n",
    "\n",
    "print(f\"DROID-SLAM initialized (opt_intr={opt_intr})\")\n",
    "print(f\"Processing {len(frame_paths)} frames...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed frames to DROID-SLAM\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, path in enumerate(tqdm(frame_paths, desc=\"Tracking\")):\n",
    "    image = cv2.imread(str(path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    timestamp = float(i) / 60.0  # 60 FPS\n",
    "    droid.track(timestamp, image)\n",
    "\n",
    "print(\"\\nRunning global bundle adjustment...\")\n",
    "traj = droid.terminate()  # (N, 7) [tx, ty, tz, qx, qy, qz, qw]\n",
    "print(f\"Got {traj.shape[0]} poses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 4x4 pose matrices\n",
    "num_poses = traj.shape[0]\n",
    "poses_4x4 = np.zeros((num_poses, 4, 4), dtype=np.float64)\n",
    "\n",
    "for i in range(num_poses):\n",
    "    t = traj[i, :3]\n",
    "    q = traj[i, 3:]  # [qx, qy, qz, qw]\n",
    "    R = Rotation.from_quat(q).as_matrix()\n",
    "    poses_4x4[i, :3, :3] = R\n",
    "    poses_4x4[i, :3, 3] = t\n",
    "    poses_4x4[i, 3, 3] = 1.0\n",
    "\n",
    "# Visualize trajectory\n",
    "positions = poses_4x4[:, :3, 3]\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 'b-', linewidth=0.5)\n",
    "ax.scatter(positions[0, 0], positions[0, 1], positions[0, 2], c='g', s=100, label='Start')\n",
    "ax.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2], c='r', s=100, label='End')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "ax.set_title(f'DROID-SLAM Trajectory ({num_poses} poses)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Trajectory extent: X=[{positions[:,0].min():.3f}, {positions[:,0].max():.3f}]\")\n",
    "print(f\"                   Y=[{positions[:,1].min():.3f}, {positions[:,1].max():.3f}]\")\n",
    "print(f\"                   Z=[{positions[:,2].min():.3f}, {positions[:,2].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metric Scale Correction\n",
    "\n",
    "Monocular SLAM has arbitrary scale. To get metric (real-world) coordinates:\n",
    "1. Place an object of known size in the scene (e.g., a stacking cube)\n",
    "2. Measure the object in real life with calipers\n",
    "3. Estimate the object size in SLAM coordinates (from the trajectory or depth)\n",
    "4. Compute: `scale = real_size / slam_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric scale correction\n",
    "# Set these values based on your calibration object\n",
    "KNOWN_REAL_DISTANCE_M = 0.05  # e.g., 50mm cube side = 0.05m\n",
    "KNOWN_SLAM_DISTANCE = 1.0     # Measure from SLAM trajectory (update this!)\n",
    "\n",
    "# For now, use scale=1.0 (uncorrected) — update after first calibration\n",
    "USE_SCALE_CORRECTION = False\n",
    "\n",
    "if USE_SCALE_CORRECTION:\n",
    "    scale = KNOWN_REAL_DISTANCE_M / KNOWN_SLAM_DISTANCE\n",
    "    poses_4x4[:, :3, 3] *= scale\n",
    "    print(f\"Applied scale correction: {scale:.4f}\")\n",
    "else:\n",
    "    print(\"No scale correction applied (set USE_SCALE_CORRECTION=True after calibration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write Poses to Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build poses.json in StackCapture format\n",
    "poses_list = []\n",
    "for i in range(min(num_poses, len(frame_paths))):\n",
    "    poses_list.append({\n",
    "        'timestamp': float(i) / 60.0,\n",
    "        'rgbIndex': i,\n",
    "        'depth': None,\n",
    "        'transform': poses_4x4[i].tolist(),\n",
    "    })\n",
    "\n",
    "# Write poses.json\n",
    "poses_file = SESSION_DIR / 'poses.json'\n",
    "with open(poses_file, 'w') as f:\n",
    "    json.dump(poses_list, f, indent=2)\n",
    "print(f\"Wrote {len(poses_list)} poses to {poses_file}\")\n",
    "\n",
    "# Update metadata\n",
    "metadata['slamProcessed'] = True\n",
    "metadata['poseCount'] = len(poses_list)\n",
    "with open(SESSION_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Updated metadata: slamProcessed=true\")\n",
    "\n",
    "print(f\"\\nSession {SESSION_NAME} is now ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the processed session\n",
    "with open(SESSION_DIR / 'poses.json') as f:\n",
    "    loaded_poses = json.load(f)\n",
    "print(f\"Loaded {len(loaded_poses)} poses from poses.json\")\n",
    "\n",
    "# Check first and last pose\n",
    "first = np.array(loaded_poses[0]['transform'])\n",
    "last = np.array(loaded_poses[-1]['transform'])\n",
    "print(f\"\\nFirst pose translation: {first[:3, 3]}\")\n",
    "print(f\"Last pose translation: {last[:3, 3]}\")\n",
    "print(f\"Total displacement: {np.linalg.norm(last[:3, 3] - first[:3, 3]):.4f} units\")\n",
    "\n",
    "# Verify metadata\n",
    "with open(SESSION_DIR / 'metadata.json') as f:\n",
    "    meta = json.load(f)\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  captureSource: {meta.get('captureSource')}\")\n",
    "print(f\"  slamProcessed: {meta.get('slamProcessed')}\")\n",
    "print(f\"  poseCount: {meta.get('poseCount')}\")\n",
    "print(f\"  rgbFrameCount: {meta.get('rgbFrameCount')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}